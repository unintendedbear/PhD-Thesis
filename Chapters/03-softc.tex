\myChapter{Applied methods... title TBC}\label{chap:softc} 
\chaptermark{Applied methods... title TBC} 

\begin{flushright}{\slshape
    ...} \\ \medskip
    --- {}
\end{flushright}

\minitoc\mtcskip
\vfill

\lettrine{S}{ince} the final outcome from the application of the methodology that is proposed in this thesis is a set of rules, that will aid the CSOs in the process of reviewing and extending the set of security policies, we present the most suitable techniques to produce this outcome, taking into account the input data. The input data are be the actions that the employees make with their devices in certain situations, hence the final problem is that of classification, to see if they should be allowed or not.

Therefore, in this chapter we will look over the techniques that take part in the process, from selecting the data in the database to actually obtaining the set of rules and visualise them, via data preprocessing and transformation. Furthermore, the soft computing techniques that have been applied in research for the problem of classification -- extraction of rules -- and visualisation are also detailed.

It is important to note that this process, known as ``Knowledge Discovery in Databases'' is not the proposed methodology itself, but part of it. These are the necessary steps and methods to obtain the rules, but the proposed algorithms can be used or not depending on the use case, which will be defined later.

\section{The Knowledge Discovery in Databases process}

The process of discovering new -- previously unknown or non trivial -- relationships between the instances of the data, being these the observations of the use case in certain environments, has been named ``Knowledge Discovery in Databases'' or KDD \cite{fayyad1996data}. Every step of the process is described in \cite{fayyad1996data}, where the authors also remark that the Data Mining procedure is part of KDD. This is because even though DM is the most famous and indeed is a very importante step, the rest are equally important and sometimes more, because without a proper preprocessing or interpretation of the results, DM could lead to misunderstandings.

This way, the steps detailed in \cite{fayyad1996data} are the following:

\begin{enumerate}
	\item Identifying the desired outcome or use of the process by the final user.
	\item Obtaining the set of data to explore.
	\item Cleaning and preprocessing of the data.
	\item Transforming the data in order to obtain new attributes of the observations or reduce them.
	\item Identifying the methods to use according to what has been defined in step 1.
	\item Applying the methods.
	\item Presenting the results in a suitable way for the final user.
\end{enumerate}

In the succeeding sections, we will describe the most used techniques for every case.

\section{Data selection}

When we talk about selecting the target data, we refer to choosing the set of data to which we want to apply the different, either if is the whole set of observations or a subset of it. The reasons for using all the data at hand or just a portion will entirely depend on the aim of the use case, and whether we want to generaly discover new relationships between the observations or just to study a particular group or time of day. For this reason, there is not a particular technique that can be used in this step, as it consists of gathering the data from the source of the observations.

Therefore, this is a matter of properly designing the database, and it is more important to do so for scalability reasons, when the number of observations that will be stored tend to be treated the same way as ``Big Data'' \cite{begoli2012design, wu2014data}.

\section{Data preprocessing}

The preprocessing of the data includes the treatment of some of the values, the cleaning of the database, and the application of balancing techniques.

On the one hand, the data might not be directly stored in the way that it is needed for the knowledge discovery. This means that either the information is in log files from what the observations have to be extracted and stored in a database, or even if the data is already in a database, the attributes related with the observation might be distributed along many different tables. What is more, the assignation of a \textit{class}, i.e. a label which describes the group to which an observation belongs, can be made manually or by performing techniques such as clustering \cite{witten2016data}. In addition, new attributes can be added to the observation by processing the ones obtained directly from the sensor, and this way we add information that might be helpful. Then, this preprocessing and actual making of the dataset is an adaptable task, because it can be done with any chosen tool. However, this is a process that can be avoided with a careful definition of the values of interest and the structure of the database that will store them.

On the other hand, the observation of the real world can lead to missing or repeated values due to failing sensors or the communication with them. This is why it is important to perform a database cleaning. The work discussed in \cite{wilson2001maintaining} presents an exhaustive review of works which study database cleaning and their conclusion is that a database with good quality is decisive when trying to obtain good accuracies when classifying new observations; a fact which was also demonstrated in \cite{zeineb2014thesis}. This means that the model that will be built in the data mining step will be more helpful, and more knowledge can be extracted, when the database is well maintained.

Many cleaning techniques have been proposed in literature \cite{wilson2001maintaining} in order to guarantee the good quality of
a given dataset. Most of these techniques are based on updating a database by adding or deleting instances to optimize and reduce the initial database. These policies include different operations such as deleting the outdated, redundant, or inconsistent instances; merging groups of objects to eliminate redundancy and improve reasoning power; re-describe objects to repair incoherencies; check for signs of corruption in the database and controlling any abnormalities in the database which might signal a problem. Working with a database which is not cleaned can become sluggish and without accurate data users will make uninformed decisions.

Lastly, it is also usual to have an unequal number of observations in every class or group. This is called ``data imbalance'' \cite{imbalanced_data_05}. In order to deal with this problem there exist several methods in the literature, but all of them are mainly grouped in three techniques \cite{imbalance_techniques_02}: 

\begin{itemize}
\item \textit{Undersampling the majority classes}: i.e. reduce the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the minority classes}: i.e. introduce additional -- normally synthetic -- patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class} to compensate for the imbalance ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

The first option has been applied in some works, following a random undersampling approach \cite{random_undersampling_08}, but it has the problem of the loss of valuable information. The second has been so far the most widely used, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique) \cite{smote_02}, a method proposed by Chawla et al. for creating ``artificial'' samples for the minority class, in order to balance the amount of them with respect. However this technique is based in numerical computations, which consider different distance measures, in order to generate useful patterns , i.e. realistic or similar to the existing ones.

The third option implies using a method in which a cost can be associated to the classifier accuracy at every step. This was done for instance by Alfaro-Cid et al. in \cite{cost_adjustment_07}, where they used a Genetic Programming (GP) approach in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative -- an element from the minority class was classified as belonging to the majority class --. However almost all the approaches deal with numerical data, which is important to take into account when working with nominal -- not integer or real -- values as well.

\section{Data transformation}

Even though the aforementioned balancing techniques can be considered as ``data transformation'', in \cite{fayyad1996data} they include as this kind of techniques the ones that reduce the space of attributes, so that the processing is less computationally expensive. This is often named ``feature reduction'' or ``feature selection''.

Feature reduction is a main point of interest across a wide variety of fields and focusing on this step is crucial as it often presents a source of significant information loss. Many techniques were proposed in literature to achieve the task of feature reduction and they can be categorized into two main heads; techniques that transform the original meaning of the features, called the ``transformation-based approaches", and the second category is a set of semantic-preserving techniques known as the ``selection-based approaches".

Transformation based approaches, also called ``feature extraction approaches", involve simplifying the amount of resources required to accurately describe a large set of data. Feature extraction is a general term for methods that construct combinations of variables to represent the original set of features but with new variables while still describing the data with sufficient accuracy. The transformation based techniques are employed in situations where the semantics of the original database will not be needed by any future process. In contrast to the semantics-destroying dimensionality reduction techniques, the semantics-preserving techniques, also called ``feature selection techniques", attempt to retain the meaning of the original
feature set. The main aim of this kind of techniques is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features \cite{liu1998feature}. In this work, we mainly focus on the use of a feature selection technique, instead of a feature extraction technique, as it is crucial to preserve the semantics of the features in the URL data that we dispose at hand, and among them, select the most important/informative ones which nearly preserve  the same performance as the initial feature set.

Yet it is important to mention that most feature selection techniques proposed in the literature suffer from some limitations. Most of these techniques involve the user for the task of the algorithms parameterization and this is  seen as a significant drawback. Some feature selectors require noise levels to be specified by the user beforehand, some simply rank features leaving the user to choose their own subset. There are those that require the user to state how many features are to be chosen, or they must supply a threshold that determines when the algorithm should terminate. All of these require the user to make a decision based on its own (possibly faulty) judgment \cite{jensen2005semantics}. To overcome the shortcomings of the existing methods, it would be interesting to look for a method that does not require any external or additional information to function appropriately. Rough Set Theory (RST) \cite{pawlak2008rough} can be used as such a tool to discover data dependencies and to reduce the number of attributes contained in the URL dataset using the data alone, requiring no additional information \cite{jensen2005semantics}.

\section{Soft computing techniques applied to data mining and visualisation}

Until now, for the other parts of the process, we have focused in some characteristics of the dataset, such as the number of attributes, whether it has missing values, or the difference between cases belonging to each class. But the type of data for every attribute is also important, because it will determine the kind of algorithms that can be used.

This way, an attribute can be \textit{numeric} or \textit{nominal} \cite{witten2016data}. Numeric attributes measure continuous values such as integers and real numbers, and boolean as well, whilst nominal attributes -- also named \textit{categorical} -- take their value from a predefined, finite set of possibilities. In what follows we will overview the algorithms that can be used in the cases that, like ours, the data is mostly nominal.

\subsection{Classification in the data mining process}

Inside KDD, the process of classification, or application of classifying algorithms, helps in building a model of the data set, and to understand the relationships therein. As previously said, the data coming from BYOD practises is usually not only numerical or nominal, thus, only classification algorithms that support both types of data can be considered. Weka \cite{weka:site} is a collection of State-of-the-Art machine learning algorithms and data preprocessing tools that are key for data mining processes \cite{witten2016data}. On the other hand, it is important that for our purposes we focus on rule-based and decision-tree-based algorithms. A decision-tree algorithm is a group of conditions organised in a top-down recursive manner in a way that a class is assigned following a path of conditions, from the root of the tree to one of its leaves. Generally speaking, the possible classes to choose are mutually exclusive. Furthermore, these algorithms are also called ``divide-and-conquer'' algorithms. On the other hand, there are the ``separate-and-conquer'' algorithms, which work creating rules one at a time, then the instances covered by the created rule are removed and the next rule is generated from the remaining instances. The most important characteristic of these algorithms is that the model that is built from the dataset is expressed in the form of a set of rules.

Inside the rule-based and decision tree-based algorithms, there is a great number of possible algorithms to work with, we have conducted a preselection phase trying to choose those which would yield better results in the experiments. A reference to each Weka classifier can be found at \cite{witten2016data}. Below are described the top five techniques, obtained from the best results  of the experiments done in this stage, along with more specific bibliography. Na\"{i}ve Bayes method \cite{Bayesian_Classifier_97} has been included as a baseline, normally used in text categorization problems. According to the results, the five selected classifiers are much better than this method.

\begin{description}
  \item[Na\"{i}ve Bayes] It is the classification technique that we have used as a reference for either its simplicity and ease to understand. Its basis relies on the Bayes Theorem and the possibility of represent the relationship between two random variables as a Bayesian network \cite{rish2001empirical}. Then, by assigning values to the variables probabilities, the probabilities of the occurrences between them can be obtained. Thus, assuming that a set of attributes are independent one from another, and using the Bayes Theorem, patterns can be classified without the need of trees or rule creation, just by calculating probabilities.
   \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \cite{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. In this way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes. 
   \item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \cite{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.
   \item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \cite{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided in two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \cite{Elomaa2001}.
   \item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e., the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \cite{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).
   \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \cite{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description} 
 
Another SC technique that can be used for classification is Genetic Programming (GP), which has been proposed in literature for dealing with the problem of discovering novel, interesting knowledge and rules from large amounts of data \cite{freitas2002data}, given that the up-to-date approaches are based in general pre-defined of manually defined rules \cite{ali2015analysis}. Considered part of the so-called \emph{Evolutionary Algorithms} \cite{back1996evolutionary}, GP is an optimization technique inspired by natural evolution. One of the advantages of GP is that by making the solutions to a problem internally encoded as trees, they themselves can be seen as decision tree classifiers \cite{safavian1990survey} and can be expressed as a set of rules.

The GP approach also has the advantage of being novel. To the best of our knowledge, there is still not a tool that helps CSOs in
developing new security rules via GP, even as this method has been indeed applied to classification, as described by Espejo et al. in \cite{espejo2010survey}. In fact, their survey theoretically supports our decision of applying GP to obtain security rules in a BYOD environment.

In our case, the assigned classes or action to take -- right part of the security rule -- would be the leaves of the tree, whilst the nodes are the conditions that have to be met to apply the class -- left part of the rule -- . Taking this into account, GP can be used to generate these classification trees, optimising an objective function called {\em fitness}. In this case the fitness can be defined as the accuracy of a rule or set of rules, being this the most used metric in classification \cite{witten2016data}, along with the classification error. But since there are other metrics that influence in ``how good'' a rule or a set of rules is, such as the depth of the created tree, the number of nodes it has, or the obtained false positives
\cite{back1996evolutionary}, it would be of convenience to use them in the definition of the fitness.

The coding of the individual might take two approaches, named \textit{Pittsburgh} and \textit{Michigan} \cite{freitas2002data}. The Pittsburgh uses GP to create an individual tree that model a set of different rules, given that the problem can be seen as
a classification one and therefore the model can be a decision tree \cite{safavian1990survey}. The second approach, called Michigan approach, assigns a single rule to every individual. The rule can be expressed thus as a list of conditions, with a fixed class, obtaining just one rule per execution. That means we are not using GP in this case, because the generated individual is not a tree, but a vector, so we are applying a regular Genetic Algorithm (GA) instead.

Indeed, each approach has its advantages and disadvantages. The Pittsburgh approach allows to directly obtain a set of rules able to classify instances of every existing class, meanwhile Michigan approach's solution is coded as a single rule, so that we obtain as many rules as classes are defined. The possibility of having many rules for every class instead of just one, more general, per class might seem to better help the CSO in detecting specific dangerous situations.
At the same time, obtaining a set of rules as solution is more computationally expensive due to the need of longer evaluations. Lastly, to evaluate a single rule not taking into account how it interacts with what the others cover \cite{freitas2002data} can lead to massive overlapping with the consequent loss of efficiency. 

\subsection{Data visualisation and interpretation}

In the last step of KDD, the authors in \cite{fayyad1996data} indicate that once the algorithms have been applied, the extracted knowledge should be presented and used in a system, or simply included in reports or analysis, always depending on the desired results and final users.

Furthermore, the whole process can be iterative, in the way that by applying a data visualisation algorithm such as clustering, the outcome can be used to help in building a new, smaller dataset, with the data that has been found interesting. Also, any conflict between extracted knowledge in different iterations must be resolved.

Clustering algorithms belong to what has been called ``unsupervised learning'' \cite{hartigan1975clustering}. In this case, the aim is not necessarily to create a model for classifying further and new instances, but to discover how the observations fit into groups naturally. However, after the groups or clusters have been formed, a model based on rules can be created so that we know to which cluster the new instances or observations belong.

The process of forming the clusters is by measuring how alike the observations are. The algorithm k-means \cite{hartigan1975clustering} has been traditionally used for creating \textit{k} clusters, and filling them depending on the Euclidean distances between the center of the clusters and the rest of the data. Next, the \textit{mean} of the instances is calculated and set as the new center of every cluster, and the rest of the data ir re-assigned again. The process finishes when there are no changes in the clusters.

Another tool that can be used for interpretation, as well as for classification, is \textit{association learning}. Association learning is also useful when the classes have not been specified. This kind of algorithms \cite{hipp2000algorithms} can predict not only the class but any attribute, which is why they are also used to extract interesting knowledge more than for only predicting. The same way the goodness of a classifier model is expressed by their accuracy over a test set of data, the association rules are accompanied by their \textit{confidence}, meaning te percentage of the observations over the total that the rule represents correctly. And therefore, by discovering the associations among the instances, one can visualise the relationships between the observations, similarly to what the clusters do.